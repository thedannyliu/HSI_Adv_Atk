# HyperForensics++: A Framework for Hyperspectral Image Forgery Detection and Adversarial Attacks

![Demo Visualization](visualization.png)

## Overview

HyperForensics++ is a comprehensive framework for hyperspectral image (HSI) forgery detection and adversarial attack research. This project provides a systematic environment for evaluating the robustness of forgery detection models and developing advanced adversarial attack methods tailored for hyperspectral imagery.

The framework enables researchers and practitioners to:
- Train and evaluate state-of-the-art hyperspectral forgery detection models
- Generate adversarial examples using multiple attack strategies
- Test model robustness against various attack domains (spatial, spectral, and hybrid)
- Evaluate both segmentation performance and attack effectiveness using comprehensive metrics

## Key Features

- **Complete Training Pipeline**: End-to-end training and evaluation workflow for HSI forgery detection models
- **Advanced Attack Methods**: Implementation of multiple adversarial attack algorithms including FGSM, PGD, CW, and DeepFool
- **Domain-Specific Attacks**: Specialized attacks targeting spatial domain, spectral domain, or hybrid approaches
- **Hyperspectral Data Processing**: Specialized data loading and preprocessing for hyperspectral imagery
- **Comprehensive Evaluation**: Detailed segmentation and adversarial attack evaluation metrics
- **Visualization Tools**: Rich visualization capabilities for understanding detection results and attack effects
- **Configurable Framework**: Highly customizable through configuration files

## Attack Effectiveness Example (Sample Result)

Here's an example showcasing the effectiveness of an attack generated by this framework:

- **Attack Success Rate (ASR)**: 1.0000 (30/30 successful attacks)
- **Average L2 Norm**: 105.5481
- **Average L∞ Norm**: 0.1122
- **Average SSIM**: 0.8924
- **Average LPIPS**: 0.1003
- **Average Max Band Difference**: 0.0045
- **Most Affected Bands**: 35, 36, 37, 32, 34
- **Significant Prediction Changes**:
    - 0_to_1 (Genuine to Forgery): 98.96%
    - 1_to_0 (Forgery to Genuine): 1.04%

**Impact on Segmentation Performance**:

| Metric             | Original | Adversarial | Change Rate |
|--------------------|----------|-------------|-------------|
| mIoU               | 0.5062   | 0.2094      | -58.63%     |
| Pixel Accuracy     | 0.9211   | 0.4027      | -56.28%     |
| Forgery Accuracy   | 0.4544   | 0.7213      | +58.74%     |

**Class Confusion Analysis**:
- Class 0 → Class 1: 52.24%
- Class 1 → Class 0: 0.00%


## Directory Structure

```
HyperForensics++/
├── config/             # Configuration files (train_config.yaml, attack_config.yaml, test_config.yaml)
├── models/             # Model definitions
├── datasets/           # Dataset handling and loading
├── attacks/            # Adversarial attack implementations
│   ├── basic_attacks.py      # Basic attack methods (FGSM, PGD)
│   ├── spatial_attacks.py    # Spatial domain attacks
│   ├── spectral_attacks.py   # Spectral domain attacks
│   ├── hybrid_attacks.py     # Combined spatial and spectral attacks
│   ├── attack_utils.py       # Attack utility functions
│   └── utils.py              # General utility functions for attacks
├── utils/              # Utility functions
├── metrics/            # Evaluation metrics implementations
├── scripts/            # Script tools
│   ├── train.py              # Training script
│   ├── evaluate.py           # Evaluation script
│   ├── generate_adversarial.py # Adversarial sample generation
│   └── visualization.py      # Visualization utilities
├── checkpoints/        # Model checkpoints saved during training
├── logs/               # Training logs
└── results/            # Output results directory (evaluation results, adversarial examples, visualizations)
```

## Installation

### Prerequisites

- Python 3.7+
- PyTorch 1.8+
- CUDA (for GPU acceleration)

### Environment Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/HSI_Adv_Atk.git
cd HSI_Adv_Atk/HyperForensics++

# Create a virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## Training Forgery Detection Models

### Training Process

The `scripts/train.py` script handles the model training process.

```bash
# Training with all configurations defined in train_config.yaml
python scripts/train.py --config_path ./config/train_config.yaml

# Training with specific configuration(s) from train_config.yaml
python scripts/train.py --config_path ./config/train_config.yaml --configs config1

# Training with multiple specific configurations
python scripts/train.py --config_path ./config/train_config.yaml --configs config1 config2

# Resuming training from a checkpoint
python scripts/train.py --config_path ./config/train_config.yaml --configs config1 --checkpoint ./logs/weights/exp_.../checkpoint_epoch_50.pth
```

### Configuration

- Training parameters (learning rate, batch size, epochs, optimizer, scheduler, etc.) are defined in `config/train_config.yaml`.
- You can define multiple training configurations within the YAML file.
- Use the `--configs` argument to select specific configurations to run. If omitted, all configurations are trained. The "Origin" configuration is always included for validation.

### Checkpoint Saving

The training process automatically saves the following model weights in the `logs/weights/exp_{timestamp}_...` directory:
- `best_miou.pth`: Model weights achieving the best mean Intersection over Union (mIoU) on the validation set.
- `best_acc.pth`: Model weights achieving the best pixel accuracy on the validation set.
- `last.pth`: Model weights from the final epoch of training.
- `checkpoint_epoch_X.pth`: Checkpoints saved periodically (e.g., every 5 epochs) for resuming training.

### Evaluation During Training

During training, models are evaluated on the validation set using segmentation metrics:
- **Pixel Accuracy**: Percentage of correctly classified pixels overall.
- **Mean IoU (Intersection over Union)**: Average IoU across forgery and non-forgery classes. A key metric for segmentation quality.
- **Class F1 Score**: Harmonic mean of precision and recall for each class, providing a balanced measure of detection performance.

## Adversarial Attack Generation

### Generating Adversarial Examples

The `scripts/generate_adversarial.py` script generates adversarial HSI examples using various methods and domain targets.

```bash
# Basic usage (PGD attack in spatial domain with default parameters)
python scripts/generate_adversarial.py --attack_method pgd --attack_domain spatial

# Detailed usage with custom parameters
python scripts/generate_adversarial.py --attack_method fgsm --attack_domain spectral --eps 0.1 --batch_size 4 --save_dir results/adversarial_fgsm_spectral

# Example: DeepFool attack in the spectral domain targeting GPU 0
python scripts/generate_adversarial.py --attack_method deepfool --attack_domain spectral --gpu_id 0 --eps 0.3

# Example: CW attack in the hybrid domain with custom steps and spatial weight
python scripts/generate_adversarial.py --attack_method cw --attack_domain hybrid --steps 100 --spatial_weight 0.7
```

### Configuration

- Default attack parameters are defined in `config/attack_config.yaml`.
- Command-line arguments override the configuration file settings.

```yaml
# Example snippet from config/attack_config.yaml
ATTACK:
  # Common parameters
  EPS: 0.1             # Maximum perturbation size (normalized domain)
  STEPS: 40            # Iterations for iterative attacks (PGD, CW, DeepFool)
  RANDOM_START: True   # Use random start for PGD

  # Specific attack parameters
  PGD:
    ALPHA: 0.01        # Step size for PGD
  CW:
    C: 0.001           # Confidence parameter C
    KAPPA: 10.0        # Confidence kappa
    LR: 0.01           # Learning rate for optimization
  DEEPFOOL:
    MAX_ITER: 50       # Maximum iterations
    OVERSHOOT: 0.02    # Perturbation overshoot factor

  # Domain-specific parameters
  SPECTRAL:
    TARGET_BANDS: null       # null: Auto-select based on importance, or list [10, 20, 30]
    SPECTRAL_THRESHOLD: 0.7  # Cumulative importance threshold for band selection
  HYBRID:
    SPATIAL_WEIGHT: 0.5      # Weight of spatial component (0-1)
```

### Key Parameters & Options

- `--attack_method`: Choose from `fgsm`, `pgd`, `deepfool`, `cw`.
- `--attack_domain`: Choose from `spatial`, `spectral`, `hybrid`.
- `--eps`: Maximum perturbation size (L∞ norm in normalized domain).
- `--steps`: Number of iterations for iterative attacks.
- `--adaptive_eps`: Dynamically adjusts perturbation magnitude based on data range (default: enabled). Crucial for HSI.
- `--perceptual_constraint`: Apply constraints to maintain visual similarity (default: enabled).
- `--spatial_weight`: Weight for the spatial component in `hybrid` attacks (0 to 1).
- `--target_bands`: Specify bands for `spectral` or `hybrid` attacks (e.g., "10,20,30") or use automatic selection based on `--spectral_importance`.
- `--spectral_importance`: Use spectral importance analysis (e.g., gradient-based) to select target bands (default: enabled).
- `--spectral_threshold`: Cumulative importance threshold for automatic band selection (default: 0.7).

### Supported Attack Methods

- **FGSM (Fast Gradient Sign Method)**: Single-step attack based on gradient sign. Fast but often less effective.
- **PGD (Projected Gradient Descent)**: Iterative version of FGSM, projecting perturbations back into the allowed ε-ball. Generally more effective than FGSM.
- **CW (Carlini-Wagner)**: Optimization-based attack finding minimal perturbations for misclassification. Powerful but computationally expensive.
- **DeepFool**: Iteratively finds the minimal perturbation to cross the decision boundary. Effective for finding small perturbations.

### Attack Domains

- **Spatial Domain**: Perturbs the spatial structure consistently across all spectral bands. Aims to disrupt spatial features used for detection.
- **Spectral Domain**: Modifies spectral signatures, targeting specific bands while preserving relative spatial structure. Exploits spectral vulnerabilities.
- **Hybrid Domain**: Combines spatial and spectral perturbations. Often the most effective by exploiting vulnerabilities in both dimensions. The balance is controlled by `spatial_weight`.

### Implementation Details & Enhancements

- **Data Range Adaptive Amplification**: Perturbation strength is dynamically scaled (20x-100x) based on the HSI data range, significantly improving attack effectiveness on high-dynamic-range data.
- **Unified Normalized Domain**: All calculations are performed in the [0, 1] normalized domain to prevent precision loss. Perturbation magnitudes are reported in both normalized and original scales.
- **Segmentation Model Support**: Attacks are adapted for segmentation models ([B, C, H, W] output), using spatial averaging for confidence and modified gradients.
- **Numerical Stability**: Safeguards against zero-division and near-zero gradients improve robustness, especially for DeepFool.
- **Enhanced Diagnostics**: Detailed output includes L∞/L2 norms (normalized/original), attack success rate, and band/pixel selection ratios for better analysis.

```python
# Example internal call structure (simplified)
# FGSM Spectral
adv_images = fgsm_spectral_attack(model, images, labels, eps=0.1, criterion=loss_fn, device='cuda', mean=mean, std=std, target_bands=important_bands)

# PGD Spatial
adv_images = pgd_spatial_attack(model, images, labels, eps=0.1, alpha=0.01, steps=40, criterion=loss_fn, device='cuda', mean=mean, std=std)

# DeepFool Hybrid
adv_images = deepfool_hybrid_attack(model, images, num_classes=2, max_iter=50, overshoot=0.02, device='cuda', mean=mean, std=std, spatial_weight=0.6)

# CW Hybrid
adv_images = cw_hybrid_attack(model, images, labels, c=0.001, kappa=10.0, steps=200, lr=0.01, device='cuda', mean=mean, std=std, spatial_weight=0.5)
```

### Adversarial Attack Evaluation Metrics

The success and characteristics of the generated adversarial examples are measured using:
- **Attack Success Rate (ASR)**: Percentage of samples successfully misclassified by the target model.
- **Perturbation Norms**: L0, L1, L2, L∞ norms quantify the magnitude of the added perturbation. Lower norms generally mean stealthier attacks. Reported in both normalized and original data scales.
- **Signal-to-Noise Ratio (SNR)**: Measures the ratio of image signal power to perturbation power.
- **Structural Similarity (SSIM)**: Measures the perceptual similarity between the original and adversarial images. Higher SSIM indicates better stealth.
- **Learned Perceptual Image Patch Similarity (LPIPS)**: Measures perceptual distance using deep features. Lower LPIPS suggests the changes are less perceptible to humans.

## Evaluating Model Performance

### Evaluation Script

Use `scripts/evaluate.py` to evaluate model performance on either the original dataset or generated adversarial examples.

```bash
# Evaluating the model on the original test dataset
python scripts/evaluate.py --config_path ./config/test_config.yaml --mode original

# Evaluating the model on previously generated adversarial examples
python scripts/evaluate.py --config_path ./config/test_config.yaml --mode adversarial --adv_dir ./results/adversarial/pgd_spatial_.../images --adv_flist ./results/adversarial/pgd_spatial_.../filelist.txt
```

### Configuration

Evaluation settings (model path, dataset path, metrics) are specified in `config/test_config.yaml`.

### Evaluation Metrics

The evaluation script reports both **Segmentation Metrics** (Pixel Accuracy, mIoU, Class F1 Score) and **Adversarial Attack Metrics** (ASR, Perturbation Norms, SNR, SSIM, LPIPS) when evaluating adversarial examples. This provides a comprehensive view of both the model's robustness and the attack's effectiveness.

## Visualization

### Visualization Script

The `scripts/visualization.py` script generates visual comparisons between original and adversarial images, along with perturbation details.

```bash
# Visualize adversarial examples from a specific directory
python scripts/visualization.py --input_dir ./results/adversarial/pgd_spatial_.../images --output_dir ./results/visualizations/pgd_spatial_...
```

### Visualization Outputs

The script generates several types of visualizations saved in the specified output directory:
- **RGB Comparison**: Side-by-side false-color RGB representation of the original and adversarial HSI.
- **Perturbation Map**: Visualization of the added adversarial noise, often scaled for visibility.
- **Prediction Difference Map**: Highlights pixels where the model's prediction changed after the attack.
- **Spectral Signature Plots**: Compares spectral signatures of selected pixels (original vs. adversarial) to show spectral modifications.

## Best Practices for Repository Management

For sustainable management of this project repository:

1. **Feature Branches**:
   - Create new branches only for complete features or significant changes
   - Use naming convention `feature/feature-name` or `fix/issue-description`
   - Merge to main only after testing

2. **Use .gitignore**:
   - Exclude generated files and large outputs
   - Example contents for .gitignore:
   ```
   results/adversarial/
   *.pth
   *.pt
   *.npy
   ```

3. **Git LFS for Large Files**:
   - For version-controlled large files, use Git Large File Storage
   - `git lfs install`
   - `git lfs track "*.pth" "*.npy"`

4. **Version Tagging**:
   - Tag important versions: `git tag -a v1.0.0 -m "Stable release 1.0.0"`
   - Push tags: `git push origin --tags`

5. **Clean Commit Messages**:
   - Format: `[Module] Brief description (under 50 chars)`
   - Example: `[DeepFool] Fix gradient computation and segmentation model support`

## Citation

If you use the HyperForensics++ framework in your research, please consider citing:

```
@article{your_paper,
  title={HyperForensics++: A Framework for Hyperspectral Image Forgery Detection and Adversarial Attacks},
  author={Your Name},
  journal={Your Journal},
  year={2023}
}
```

## License

This project is licensed under the MIT License. 